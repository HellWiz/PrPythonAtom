{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "#импортируем необходимые модули\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_hat, y, thresh):\n",
    "    #TODO recall\n",
    "    return sum((((y - 1) * 2) + 1) == (y_hat > thresh)) / sum(y)\n",
    "    \n",
    "def precision(y_hat, y, thresh):\n",
    "    #TODO\n",
    "    return sum((((y - 1) * 2) + 1) == (y_hat > thresh)) / sum(y_hat > thresh)\n",
    "def accuracy(y_hat, y, thresh):\n",
    "    #TODO\n",
    "    return sum(y == (y_hat > thresh)) / len(y)\n",
    "    \n",
    "    \n",
    "def f1(y_hat, y, thresh):\n",
    "    #TODO\n",
    "    return 2 * recall(y_hat, y , thresh) * precision(y_hat, y, thresh) / (recall(y_hat, y , thresh) + precision(y_hat, y, thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 passed\n",
      "Test 1 passed\n",
      "Test 2 passed\n",
      "Test 3 passed\n",
      "Test 4 passed\n",
      "Test 5 passed\n",
      "Test 6 passed\n",
      "Test 7 passed\n",
      "Test 8 passed\n",
      "Test 9 passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, precision_score, accuracy_score, f1_score\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Test {} passed\".format(i))\n",
    "    y_hat = np.random.random(30)\n",
    "    y = np.random.random(30) > 0.7\n",
    "    thresh = np.random.random()\n",
    "    \n",
    "    assert abs(recall(y_hat, y, thresh) - recall_score(y_true=y, y_pred=y_hat > thresh)) < 0.1\n",
    "    assert abs(precision(y_hat, y, thresh) - precision_score(y_true=y, y_pred=y_hat > thresh)) < 0.1\n",
    "    assert abs(accuracy(y_hat, y, thresh) - accuracy_score(y_true=y, y_pred=y_hat > thresh)) < 0.1\n",
    "    assert abs(f1(y_hat, y, thresh) - f1_score(y_true=y, y_pred=y_hat > thresh)) < 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''y_hat = #TODO предсказания, чтобы выбить recall 0.9 плохим классификатором\n",
    "y = #TODO метки, чтобы выбить recall 0.9 плохим классификатором\n",
    "t = #TODO порог, чтобы выбить recall 0.9 плохим классификатором\n",
    "'''\n",
    "#assert recall(y_hat, y, t) > 0.9\n",
    "'''\n",
    "y_hat = #TODO предсказания, чтобы выбить precision 0.9 плохим классификатором\n",
    "y = #TODO метки, чтобы выбить precision 0.9 плохим классификатором\n",
    "t = #TODO порог, чтобы выбить precision 0.9 плохим классификатором\n",
    "'''\n",
    "#assert precision(y_hat, y, t) > 0.9\n",
    "\n",
    "y_hat = np.array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])#TODO предсказания, чтобы выбить accuracy 0.9 плохим классификатором\n",
    "y = np.array([1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])#TODO метки, чтобы выбить accuracy 0.9 плохим классификатором\n",
    "t = 0.5#TODO порог, чтобы выбить accuracy 0.9 плохим классификатором\n",
    "\n",
    "assert accuracy(y_hat, y, t) > 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функция потерь для логистической регрессии (из материалов курса по нейронным сетям)\n",
    "\n",
    "Давайте получим функцию потерь аналогичным образом, как мы это сделали с линейной регрессией.\n",
    "\n",
    "\n",
    "логарифм правдоподобия\n",
    "$$\n",
    "\\large \n",
    "\\begin{array}{rcl} \n",
    "    \\log P\\left(\\vec{y} \\mid X, \\vec{w}\\right) \n",
    "        &=& \\log \\prod_{i=1}^{\\ell} P\\left(y = y_i \\mid \\vec{x_i}, \\vec{w}\\right) \\\\ \n",
    "        &=& \\log \\prod_{i=1}^{\\ell} \\sigma(y_i\\vec{w}^T\\vec{x_i}) \\\\ \n",
    "        &=& \\sum_{i=1}^{\\ell} \\log \\sigma(y_i\\vec{w}^T\\vec{x_i}) \\\\ \n",
    "        &=& \\sum_{i=1}^{\\ell} \\log \\frac{1}{1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}} \\\\ \n",
    "        &=& - \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}}) \n",
    "\\end{array}\n",
    "$$\n",
    "который и будем оптимизировать. \n",
    "\n",
    "Данная функция потерь \n",
    "$$\n",
    "\\large L(\\vec{x},\\vec{y},\\vec{w}) = \\sum_{i=1}^{\\ell} \\log (1 + \\exp^{-y_i\\vec{w}^T\\vec{x_i}})\n",
    "$$\n",
    "называется логистичской функцией потерь или сокращенно logloss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дано:\n",
    "- X - обучающее множество\n",
    "- Y - метки классов для обучающего множества\n",
    "- $\\eta$ - скорость обучения\n",
    "\n",
    "Алгоритм:\n",
    "1. инициализируем $\\vec{w}_0$ небольшими случайными значениями\n",
    "2. выбираем случайный объект $x_i$ с меткой $y_i$\n",
    "3. обновляем веса $ \\vec{w}_{t+1} = \\vec{w}_{t} - \\eta \\nabla L $\n",
    "4. если условие остановки не выполнело, возвращаемся к шагу 2\n",
    "\n",
    "Выпишем градиент функции потерь:\n",
    "$$\n",
    "    \\nabla L = \\left\\{ - \\frac{y_i \\vec{x_i} }  { 1 + \\exp^{y_i\\vec{w}^T\\vec{x_i}} } + \\lambda \\vec{w} \\right\\} \n",
    "$$\n",
    "\n",
    "На практике, в качестве условия остановки, часто используют изменение функции потерь на тестовой выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "selector = iris.target != 0 # ограничемся только двумя классами\n",
    "X = iris.data[:, [1,3]][selector]  # и двумя переменными\n",
    "y = iris.target[selector]\n",
    "y[y==2] = 0 # конвертируем метки классов к 1/-1\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Set1, edgecolor='k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X[:80]\n",
    "X_test = X[80:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "polytrans = PolynomialFeatures(3)\n",
    "X_train = polytrans.fit_transform(X_train)\n",
    "X_test = polytrans.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_1 = #TODO ОБучить логистическую регрессию на train с C=0.1\n",
    "pred_1 = #TODO Предсказать логистической регрессией на трейне\n",
    "pred_test = #TODO Предсказать логистической регрессией на тесте\n",
    "#TODO Сранивть метрики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO обучим логистическую регрессию с разными C и посмотрим как меняются метрики на трейне и на тесте"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пример текстовой классификации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/home/anton/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/anton/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/anton/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import grid_search\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2 . take around 10,000 640x480 pictures .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i downloaded a trial version of computer assoc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the wrt54g plus the hga7t is a perfect solutio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i dont especially like how music files are uns...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was using the cheapie pail ... and it worked...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  1\n",
       "0          2 . take around 10,000 640x480 pictures .  1\n",
       "1  i downloaded a trial version of computer assoc...  1\n",
       "2  the wrt54g plus the hga7t is a perfect solutio...  1\n",
       "3  i dont especially like how music files are uns...  0\n",
       "4  i was using the cheapie pail ... and it worked...  1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('products_sentiment_train.tsv', sep='\\t', header=None)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "porter_stemmer = PorterStemmer()\n",
    "for i in range(0,len(data)):\n",
    "    arr = data.iloc[i,0].split(' ')\n",
    "    data.iloc[i,0] = ' '.join([str(porter_stemmer.stem(wordnet_lemmatizer.lemmatize(arr[i]))) for i in range(0, len(arr))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier: logReg, vectoizer:  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None) , best_score:  0.776 {'classifier__C': 10, 'classifier__class_weight': 'balanced', 'classifier__max_iter': 70, 'classifier__penalty': 'l2', 'vectorizer__ngram_range': (1, 2), 'vectorizer__stop_words': None}\n",
      "Classifier: logReg, vectoizer:  TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None) , best_score:  0.787 {'classifier__C': 15, 'classifier__class_weight': None, 'classifier__max_iter': 70, 'classifier__penalty': 'l2', 'vectorizer__ngram_range': (1, 3), 'vectorizer__stop_words': None}\n"
     ]
    }
   ],
   "source": [
    "vectors = [CountVectorizer(), TfidfVectorizer()]\n",
    "parameters_grid = {\n",
    "    'vectorizer__stop_words' : [nltk.corpus.stopwords.words('english'), 'english', None],\n",
    "    'vectorizer__ngram_range' : [(1,2), (1,3)],\n",
    "    'classifier__C':[0.25,0.5,1,2,5,10,15],\n",
    "    'classifier__class_weight':['balanced', None],\n",
    "    'classifier__max_iter':[70,100,200],\n",
    "    'classifier__penalty':['l2','l1']\n",
    "}\n",
    "for vect in vectors:\n",
    "        pipeline = Pipeline([('vectorizer', vect),('classifier',LogisticRegression())])\n",
    "        grid_cv = grid_search.GridSearchCV(pipeline, parameters_grid, n_jobs=-1, scoring = 'accuracy', cv = 3)\n",
    "        grid_cv.fit(data[0],data[1])\n",
    "        print('Classifier: logReg, vectoizer: ',vect,', best_score: ',grid_cv.best_score_, grid_cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>so , why the small digital elph , rather than ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3/4 way through the first disk we played on it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>better for the zen micro is outlook compatibil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>6 . play gameboy color games on it with goboy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>likewise , i 've heard norton 2004 professiona...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                               text\n",
       "0   0  so , why the small digital elph , rather than ...\n",
       "1   1  3/4 way through the first disk we played on it...\n",
       "2   2  better for the zen micro is outlook compatibil...\n",
       "3   3    6 . play gameboy color games on it with goboy .\n",
       "4   4  likewise , i 've heard norton 2004 professiona..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = pd.read_csv('products_sentiment_test.tsv', sep='\\t')\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-884ba37e8189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mporter_stemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwordnet_lemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(data_test)):\n",
    "    arr = data_test.iloc[i,1].split(' ')\n",
    "    data_test.iloc[i,1] = ' '.join([str(porter_stemmer.stem(wordnet_lemmatizer.lemmatize(arr[i].lower()))) for i in range(0, len(arr))])\n",
    "res = clf.predict(vectorizer.transform(data_test.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Appendix TODO together. Классификация отзывов о технике на русском"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
